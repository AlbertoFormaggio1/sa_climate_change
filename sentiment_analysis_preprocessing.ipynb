{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of analyzing online pieces of writing to determine the emotional tone they carry, whether they’re positive, negative, or neutral.\n",
    "In simple words, sentiment analysis helps to find the author’s attitude towards a topic.\n",
    "We want to make a study which can be socially relevant by looking at the attitude of people toward Climate Change. This may be done by scraping tweets from Twitter.\n",
    "We will use a pre-labeled dataset available on kaggle: https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, let us import all the packages that we are going to need in our project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the data\n",
    "\n",
    "Let us first import the dataset in order to see with what kind of data we are dealing with"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sentiment                                            message  \\\n0         -1  @tiniebeany climate change is an interesting h...   \n1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n3          1  RT @Mick_Fanning: Just watched this amazing do...   \n4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n\n              tweetid  \n0  792927353886371840  \n1  793124211518832641  \n2  793124402388832256  \n3  793124635873275904  \n4  793125156185137153  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>@tiniebeany climate change is an interesting h...</td>\n      <td>792927353886371840</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n      <td>793124211518832641</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n      <td>793124402388832256</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n      <td>793124635873275904</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n      <td>793125156185137153</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitter_sentiment_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"RT @TimotheusW: Harrowing read about the relentless pursuit of #CSG in #Australia - 'Australia isnÃ¢â‚¬â„¢t Ã¢â‚¬Å“tacklingÃ¢â‚¬ï†\\x9d climate change, weÃ¢â‚¬Â¦\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'][186]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have 2 separate columns for the table id and the tweetid, let us use our ids: they will use less space than storing huge numbers.\n",
    "We will remove the column associated to the tweetids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   sentiment                                            message\n0         -1  @tiniebeany climate change is an interesting h...\n1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...\n2          1  Fabulous! Leonardo #DiCaprio's film on #climat...\n3          1  RT @Mick_Fanning: Just watched this amazing do...\n4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>@tiniebeany climate change is an interesting h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('tweetid', axis=1)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also drop the tweets which are about news and neutral since we want our model to know about the general feeling of people about global warming.\n",
    "For this reason, we will only keep the messages associated to texts with sentiment 1 or -1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87886\n",
      "53904\n"
     ]
    },
    {
     "data": {
      "text/plain": "   sentiment                                            message\n0         -1  @tiniebeany climate change is an interesting h...\n1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...\n2          1  Fabulous! Leonardo #DiCaprio's film on #climat...\n3          1  RT @Mick_Fanning: Just watched this amazing do...\n9          1  #BeforeTheFlood Watch #BeforeTheFlood right he...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>@tiniebeany climate change is an interesting h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>#BeforeTheFlood Watch #BeforeTheFlood right he...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.size)\n",
    "\n",
    "df = df[(df.sentiment != 0) & (df.sentiment != 2)]\n",
    "print(df.size)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now take a look at how the tweets are done:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tiniebeany climate change is an interesting hustle as it was global warming but the planet stopped warming for 15 yes while the suv boom\n"
     ]
    }
   ],
   "source": [
    "print(df['message'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Many tweets are also the same, we should get rid of them.\n",
    "Messages with same texts are likely to be due to some spamming bot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['message'], inplace=True)\n",
    "df['original_message'] = df['message']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "When text files are considered, there aren't many features available, we only have text.\n",
    "From there we will need to extract everything that we need to perform our classification task.\n",
    "\n",
    "One of the main problems with text data is noise. Capital letters, punctuation, links, spelling errors are some examples of problems that add noise to the data and are likely to worsen the performance of the model.\n",
    "The steps used for the cleaning of the text data will be the following:\n",
    "1) Lowercase the text\n",
    "2) Remove the punctuation\n",
    "3) Remove the stop-words\n",
    "4) Remove @mentions\n",
    "5) Removal of HTML links\n",
    "6) Spell check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lowercasing\n",
    "\n",
    "When considering comments, capital letters don't matter because they are used randomly like:\n",
    "WHATTT? i CAnT BeLIEVE ITttt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      " RT @NatGeoChannel: Watch #BeforeTheFlood right here, as @LeoDiCaprio travels the world to tackle climate change https://t.co/LkDehj3tNn httÃ¢â‚¬Â¦ \n",
      "Now it is:\n",
      " rt @natgeochannel: watch #beforetheflood right here, as @leodicaprio travels the world to tackle climate change https://t.co/lkdehj3tnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "df['message'] = df['message'].apply(str.lower)\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Punctuation Removal\n",
    "\n",
    "For the same reason as before, consider the message WHAT????????????????? R U SERIOUS ??? OMG!!!\n",
    "As you can see there's a lot of noise in the sentence.\n",
    "While it can be helpful to understand the meaning or for POS tagging, in our case punctuation is just useless and more importantly is not very needed in comments where it could be used wrongly (it's not a scientific paper or an article)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there is the @: for twitter this is a special character so we should deal with it separately.\n",
    "Also some usernames can contain '_'\n",
    "\n",
    "From twitter docs: \"A username can only contain alphanumeric characters (letters A-Z, numbers 0-9) with the exception of underscores\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?[\\]^`{|}~\n"
     ]
    }
   ],
   "source": [
    "punctuation = punctuation.replace('@','')\n",
    "punctuation = punctuation.replace('_','')\n",
    "print(punctuation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now we can move ahead by removing all the other elements. None of them seems to be relevant for our task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      " rt @natgeochannel: watch #beforetheflood right here, as @leodicaprio travels the world to tackle climate change https://t.co/lkdehj3tnn httã¢â‚¬â¦ \n",
      "Now it is:\n",
      " rt @natgeochannel watch beforetheflood right here as @leodicaprio travels the world to tackle climate change httpstcolkdehj3tnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "df['message'] = df['message'].apply(lambda x: x.translate(str.maketrans('','',punctuation)))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mentions, rt and hashtag Removal\n",
    "\n",
    "In twitter, it is possible for user to mention other people.\n",
    "This information is totally irrelevant when considering the sentiment/emotions of a text. We shall move ahead and deal with it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      " rt @natgeochannel watch beforetheflood right here as @leodicaprio travels the world to tackle climate change httpstcolkdehj3tnn httã¢â‚¬â¦ \n",
      "Now it is:\n",
      "   watch beforetheflood right here as  travels the world to tackle climate change httpstcolkdehjtnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "# re.sub will remove any substring that matches with the following regex:\n",
    "# one @, followed by any character alphanumeric including _\n",
    "# OR the #\n",
    "# OR any number of digit >=1\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'@\\w+|#\\w+|\\d+|rt', '', x))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removal of links\n",
    "\n",
    "Links as well are not very useful when talking about understanding a text, they're just a reference to something else.\n",
    "That's why we will remove anything starting with www, http or https (usually what links start with)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      "   watch beforetheflood right here as  travels the world to tackle climate change httpstcolkdehjtnn httã¢â‚¬â¦ \n",
      "Now it is:\n",
      "   watch beforetheflood right here as  travels the world to tackle climate change  \n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "\n",
    "# This regex will match anything starting with (http or www or https) followed by any character that isn't a space: therefore the whole word starting with these values will be eliminated until a space is found.\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'htt\\S+|www\\S+', '', x))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling strange characters\n",
    "\n",
    "By scraping the dataset, I found some strange characters that are very likely to be just some errors related to the wrong understanding of the encoding format.\n",
    "This problem was not solvable by changing the reading encoding, which means that the errors must be done when saving the datasets.\n",
    "I tried to copy as many of those characters as I could find while looking through the dataset.\n",
    "\n",
    "We remove the characters by simply compiling all these special characters in a regex and applying it to the message column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def handle_undefined_chars(text):\n",
    "    # handling BOM characters\n",
    "    try:\n",
    "        clean = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = text\n",
    "    l = ['ã', '¢', 'â', '‚', '¬', 'å', '“', 'ã', 'ï', '†', '', '¦','°','å','³','’','„','¹','â','è','é','ç','ò','à','§','€']\n",
    "    regex = re.compile(r'\\w*(' + '|'.join(l) + r')\\w*')\n",
    "\n",
    "    return regex.sub('', clean)\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: handle_undefined_chars(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removal of stopwords\n",
    "\n",
    "Stopwords are frequent words like 'the', 'a', 'about' which are very often used but don't provide us any useful information about the analysis of the meaning of the text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is too much: so much information would be lost in the process!\n",
    "Not, don't, no etc. are lost in the process: we are losing way too many pieces of information.\n",
    "\n",
    "For example:\n",
    "\"I am not happy\" becomes \"I am happy\"!!!\n",
    "The meaning of the sentence is totally lost!\n",
    "It's better if we define our own set of stopwords."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      "   watch beforetheflood right here as  travels the world to tackle climate change   \n",
      "Now it is:\n",
      " watch beforetheflood right travels world tackle climate change\n"
     ]
    }
   ],
   "source": [
    "my_stopwords=['a', 'about', 'above', 'after', 'again', 'all', 'am', 'an',\n",
    "              'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "              'being', 'below', 'between', 'both', 'but', 'by', 'can', 'd', 'did', 'do',\n",
    "              'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from',\n",
    "              'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "              'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "              'into', 'is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "              'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "              'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'own', 're',\n",
    "              's', 'same', 'she', \"shes\", 'should', \"shouldve\", 'so', 'some', 'such',\n",
    "              't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "              'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "              'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was',\n",
    "              'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom',\n",
    "              'why', 'will', 'with', 'won', 'u', 'y', 'you', \"youd\", \"youll\", \"youre\",\n",
    "              \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([w for w in str(text).split() if w not in my_stopwords])\n",
    "\n",
    "before = df['message'][1]\n",
    "df['message'] = df['message'].apply(lambda x: cleaning_stopwords(x))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removal of emoticon and emojis\n",
    "\n",
    "When writing texts on the internet, people often use emojis and emoticons to convey their emotions.\n",
    "Let us consider:\n",
    "- \"Today there's the snow :)\"\n",
    "- \"Today there's the snow :(\"\n",
    "The sentence is the same but the sentiment of the message is different: in the first case it's positive while in the second sentence it is negative.\n",
    "\n",
    "Of course a text analyzer is not able to understand what's the meaning of a colon followed by a parathesis, therefore it will just get rid of the information, it would be only noise.\n",
    "However, it is important to transform these emojis in such a way they can be of help for the task we're currently considering.\n",
    "The same reasoning is for emojis.\n",
    "\n",
    "At the following <a href=\"https://github.com/NeelShah18/emot/blob/master\"> github repo </a> we can find a list of emoticon and emojis which are associated to their corresponding word transcription\n",
    "\n",
    "Let us first show some examples of how this actually work:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "EMO_UNICODE = {\n",
    "    u':OK_button:': u'\\U0001F197',\n",
    "    u':OK_hand:': u'\\U0001F44C',\n",
    "    u':ON!_arrow:': u'\\U0001F51B',\n",
    "    u':anger_symbol:': u'\\U0001F4A2',\n",
    "    u':angry_face:': u'\\U0001F620',\n",
    "    u':angry_face_with_horns:': u'\\U0001F47F',\n",
    "    u':anguished_face:': u'\\U0001F627',\n",
    "    u':ant:': u'\\U0001F41C',\n",
    "    u':antenna_bars:': u'\\U0001F4F6',\n",
    "    u':anticlockwise_arrows_button:': u'\\U0001F504',\n",
    "    u':articulated_lorry:': u'\\U0001F69B',\n",
    "    u':artist_palette:': u'\\U0001F3A8',\n",
    "    u':astonished_face:': u'\\U0001F632',\n",
    "    u':atom_symbol:': u'\\U0000269B',\n",
    "    u':backhand_index_pointing_down:': u'\\U0001F447',\n",
    "    u':backhand_index_pointing_left:': u'\\U0001F448',\n",
    "    u':backhand_index_pointing_right:': u'\\U0001F449',\n",
    "    u':backhand_index_pointing_up:': u'\\U0001F446',\n",
    "    u':beating_heart:': u'\\U0001F493',\n",
    "    u':biohazard:': u'\\U00002623',\n",
    "    u':black_heart:': u'\\U0001F5A4',\n",
    "    u':black_large_square:': u'\\U00002B1B',\n",
    "    u':black_medium-small_square:': u'\\U000025FE',\n",
    "    u':black_medium_square:': u'\\U000025FC',\n",
    "    u':black_nib:': u'\\U00002712',\n",
    "    u':black_small_square:': u'\\U000025AA',\n",
    "    u':black_square_button:': u'\\U0001F532',\n",
    "    u':blossom:': u'\\U0001F33C',\n",
    "    u':blowfish:': u'\\U0001F421',\n",
    "    u':blue_book:': u'\\U0001F4D8',\n",
    "    u':blue_circle:': u'\\U0001F535',\n",
    "    u':blue_heart:': u'\\U0001F499',\n",
    "    u':boar:': u'\\U0001F417',\n",
    "    u':bomb:': u'\\U0001F4A3',\n",
    "    u':bookmark:': u'\\U0001F516',\n",
    "    u':bookmark_tabs:': u'\\U0001F4D1',\n",
    "    u':books:': u'\\U0001F4DA',\n",
    "    u':bread:': u'\\U0001F35E',\n",
    "    u':bridge_at_night:': u'\\U0001F309',\n",
    "    u':briefcase:': u'\\U0001F4BC',\n",
    "    u':bright_button:': u'\\U0001F506',\n",
    "    u':broken_heart:': u'\\U0001F494',\n",
    "    u':bug:': u'\\U0001F41B',\n",
    "    u':building_construction:': u'\\U0001F3D7',\n",
    "    u':burrito:': u'\\U0001F32F',\n",
    "    u':bus:': u'\\U0001F68C',\n",
    "    u':bus_stop:': u'\\U0001F68F',\n",
    "    u':bust_in_silhouette:': u'\\U0001F464',\n",
    "    u':busts_in_silhouette:': u'\\U0001F465',\n",
    "    u':butterfly:': u'\\U0001F98B',\n",
    "    u':cactus:': u'\\U0001F335',\n",
    "    u':calendar:': u'\\U0001F4C5',\n",
    "    u':call_me_hand:': u'\\U0001F919',\n",
    "    u':camel:': u'\\U0001F42A',\n",
    "    u':camera:': u'\\U0001F4F7',\n",
    "    u':camera_with_flash:': u'\\U0001F4F8',\n",
    "    u':camping:': u'\\U0001F3D5',\n",
    "    u':candle:': u'\\U0001F56F',\n",
    "    u':candy:': u'\\U0001F36C',\n",
    "    u':canoe:': u'\\U0001F6F6',\n",
    "    u':card_file_box:': u'\\U0001F5C3',\n",
    "    u':cat:': u'\\U0001F408',\n",
    "    u':cat_face:': u'\\U0001F431',\n",
    "    u':cat_face_with_tears_of_joy:': u'\\U0001F639',\n",
    "    u':cat_face_with_wry_smile:': u'\\U0001F63C',\n",
    "    u':chains:': u'\\U000026D3',\n",
    "    u':chart_decreasing:': u'\\U0001F4C9',\n",
    "    u':chart_increasing:': u'\\U0001F4C8',\n",
    "    u':chart_increasing_with_yen:': u'\\U0001F4B9',\n",
    "    u':cheese_wedge:': u'\\U0001F9C0',\n",
    "    u':chequered_flag:': u'\\U0001F3C1',\n",
    "    u':cherries:': u'\\U0001F352',\n",
    "    u':cherry_blossom:': u'\\U0001F338',\n",
    "    u':chestnut:': u'\\U0001F330',\n",
    "    u':chicken:': u'\\U0001F414',\n",
    "    u':children_crossing:': u'\\U0001F6B8',\n",
    "    u':chipmunk:': u'\\U0001F43F',\n",
    "    u':chocolate_bar:': u'\\U0001F36B',\n",
    "    u':church:': u'\\U000026EA',\n",
    "    u':cigarette:': u'\\U0001F6AC',\n",
    "    u':cinema:': u'\\U0001F3A6',\n",
    "    u':circled_M:': u'\\U000024C2',\n",
    "    u':circus_tent:': u'\\U0001F3AA',\n",
    "    u':cityscape:': u'\\U0001F3D9',\n",
    "    u':cloud_with_lightning:': u'\\U0001F329',\n",
    "    u':cloud_with_lightning_and_rain:': u'\\U000026C8',\n",
    "    u':cloud_with_rain:': u'\\U0001F327',\n",
    "    u':cloud_with_snow:': u'\\U0001F328',\n",
    "    u':clown_face:': u'\\U0001F921',\n",
    "    u':coffin:': u'\\U000026B0',\n",
    "    u':confetti_ball:': u'\\U0001F38A',\n",
    "    u':confounded_face:': u'\\U0001F616',\n",
    "    u':confused_face:': u'\\U0001F615',\n",
    "    u':cucumber:': u'\\U0001F952',\n",
    "    u':curly_loop:': u'\\U000027B0',\n",
    "    u':currency_exchange:': u'\\U0001F4B1',\n",
    "    u':curry_rice:': u'\\U0001F35B',\n",
    "    u':custard:': u'\\U0001F36E',\n",
    "    u':customs:': u'\\U0001F6C3',\n",
    "    u':cyclone:': u'\\U0001F300',\n",
    "    u':dagger:': u'\\U0001F5E1',\n",
    "    u':dango:': u'\\U0001F361',\n",
    "    u':dark_skin_tone:': u'\\U0001F3FF',\n",
    "    u':dashing_away:': u'\\U0001F4A8',\n",
    "    u':deciduous_tree:': u'\\U0001F333',\n",
    "    u':deer:': u'\\U0001F98C',\n",
    "    u':delivery_truck:': u'\\U0001F69A',\n",
    "    u':department_store:': u'\\U0001F3EC',\n",
    "    u':derelict_house:': u'\\U0001F3DA',\n",
    "    u':desert:': u'\\U0001F3DC',\n",
    "    u':desert_island:': u'\\U0001F3DD',\n",
    "    u':disappointed_but_relieved_face:': u'\\U0001F625',\n",
    "    u':disappointed_face:': u'\\U0001F61E',\n",
    "    u':dizzy:': u'\\U0001F4AB',\n",
    "    u':dizzy_face:': u'\\U0001F635',\n",
    "    u':dollar_banknote:': u'\\U0001F4B5',\n",
    "    u':double_exclamation_mark:': u'\\U0000203C',\n",
    "    u':elephant:': u'\\U0001F418',\n",
    "    u':face_screaming_in_fear:': u'\\U0001F631',\n",
    "    u':face_with_cold_sweat:': u'\\U0001F613',\n",
    "    u':face_with_head-bandage:': u'\\U0001F915',\n",
    "    u':face_with_medical_mask:': u'\\U0001F637',\n",
    "    u':face_with_open_mouth:': u'\\U0001F62E',\n",
    "    u':face_with_open_mouth_&_cold_sweat:': u'\\U0001F630',\n",
    "    u':face_with_rolling_eyes:': u'\\U0001F644',\n",
    "    u':face_with_steam_from_nose:': u'\\U0001F624',\n",
    "    u':face_with_stuck-out_tongue:': u'\\U0001F61B',\n",
    "    u':face_with_stuck-out_tongue_&_closed_eyes:': u'\\U0001F61D',\n",
    "    u':face_with_stuck-out_tongue_&_winking_eye:': u'\\U0001F61C',\n",
    "    u':face_with_tears_of_joy:': u'\\U0001F602',\n",
    "    u':face_with_thermometer:': u'\\U0001F912',\n",
    "    u':face_without_mouth:': u'\\U0001F636',\n",
    "    u':fearful_face:': u'\\U0001F628',\n",
    "    u':fire:': u'\\U0001F525',\n",
    "    u':fire_engine:': u'\\U0001F692',\n",
    "    u':fireworks:': u'\\U0001F386',\n",
    "    u':fish:': u'\\U0001F41F',\n",
    "    u':fish_cake_with_swirl:': u'\\U0001F365',\n",
    "    u':fishing_pole:': u'\\U0001F3A3',\n",
    "    u':five-thirty:': u'\\U0001F560',\n",
    "    u':five_o’clock:': u'\\U0001F554',\n",
    "    u':flag_in_hole:': u'\\U000026F3',\n",
    "    u':flashlight:': u'\\U0001F526',\n",
    "    u':fleur-de-lis:': u'\\U0000269C',\n",
    "    u':flexed_biceps:': u'\\U0001F4AA',\n",
    "    u':floppy_disk:': u'\\U0001F4BE',\n",
    "    u':flower_playing_cards:': u'\\U0001F3B4',\n",
    "    u':flushed_face:': u'\\U0001F633',\n",
    "    u':fog:': u'\\U0001F32B',\n",
    "    u':foggy:': u'\\U0001F301',\n",
    "    u':folded_hands:': u'\\U0001F64F',\n",
    "    u':footprints:': u'\\U0001F463',\n",
    "    u':fork_and_knife:': u'\\U0001F374',\n",
    "    u':fork_and_knife_with_plate:': u'\\U0001F37D',\n",
    "    u':fountain:': u'\\U000026F2',\n",
    "    u':fountain_pen:': u'\\U0001F58B',\n",
    "    u':four-thirty:': u'\\U0001F55F',\n",
    "    u':four_leaf_clover:': u'\\U0001F340',\n",
    "    u':four_o’clock:': u'\\U0001F553',\n",
    "    u':fox_face:': u'\\U0001F98A',\n",
    "    u':framed_picture:': u'\\U0001F5BC',\n",
    "    u':french_fries:': u'\\U0001F35F',\n",
    "    u':fried_shrimp:': u'\\U0001F364',\n",
    "    u':frog_face:': u'\\U0001F438',\n",
    "    u':front-facing_baby_chick:': u'\\U0001F425',\n",
    "    u':frowning_face:': u'\\U00002639',\n",
    "    u':frowning_face_with_open_mouth:': u'\\U0001F626',\n",
    "    u':fuel_pump:': u'\\U000026FD',\n",
    "    u':full_moon:': u'\\U0001F315',\n",
    "    u':full_moon_with_face:': u'\\U0001F31D',\n",
    "    u':funeral_urn:': u'\\U000026B1',\n",
    "    u':game_die:': u'\\U0001F3B2',\n",
    "    u':gear:': u'\\U00002699',\n",
    "    u':gem_stone:': u'\\U0001F48E',\n",
    "    u':ghost:': u'\\U0001F47B',\n",
    "    u':girl:': u'\\U0001F467',\n",
    "    u':glass_of_milk:': u'\\U0001F95B',\n",
    "    u':glasses:': u'\\U0001F453',\n",
    "    u':globe_showing_Americas:': u'\\U0001F30E',\n",
    "    u':globe_showing_Asia-Australia:': u'\\U0001F30F',\n",
    "    u':globe_showing_Europe-Africa:': u'\\U0001F30D',\n",
    "    u':globe_with_meridians:': u'\\U0001F310',\n",
    "    u':glowing_star:': u'\\U0001F31F',\n",
    "    u':goal_net:': u'\\U0001F945',\n",
    "    u':goat:': u'\\U0001F410',\n",
    "    u':goblin:': u'\\U0001F47A',\n",
    "    u':gorilla:': u'\\U0001F98D',\n",
    "    u':graduation_cap:': u'\\U0001F393',\n",
    "    u':grapes:': u'\\U0001F347',\n",
    "    u':green_apple:': u'\\U0001F34F',\n",
    "    u':green_book:': u'\\U0001F4D7',\n",
    "    u':green_heart:': u'\\U0001F49A',\n",
    "    u':green_salad:': u'\\U0001F957',\n",
    "    u':grimacing_face:': u'\\U0001F62C',\n",
    "    u':grinning_cat_face_with_smiling_eyes:': u'\\U0001F638',\n",
    "    u':grinning_face:': u'\\U0001F600',\n",
    "    u':grinning_face_with_smiling_eyes:': u'\\U0001F601',\n",
    "    u':growing_heart:': u'\\U0001F497',\n",
    "    u':guard:': u'\\U0001F482',\n",
    "    u':guitar:': u'\\U0001F3B8',\n",
    "    u':hamburger:': u'\\U0001F354',\n",
    "    u':hammer:': u'\\U0001F528',\n",
    "    u':hammer_and_pick:': u'\\U00002692',\n",
    "    u':hammer_and_wrench:': u'\\U0001F6E0',\n",
    "    u':hamster_face:': u'\\U0001F439',\n",
    "    u':handbag:': u'\\U0001F45C',\n",
    "    u':handshake:': u'\\U0001F91D',\n",
    "    u':hatching_chick:': u'\\U0001F423',\n",
    "    u':headphone:': u'\\U0001F3A7',\n",
    "    u':hear-no-evil_monkey:': u'\\U0001F649',\n",
    "    u':heart_decoration:': u'\\U0001F49F',\n",
    "    u':heart_suit:': u'\\U00002665',\n",
    "    u':heart_with_arrow:': u'\\U0001F498',\n",
    "    u':heart_with_ribbon:': u'\\U0001F49D',\n",
    "    u':hibiscus:': u'\\U0001F33A',\n",
    "    u':high-heeled_shoe:': u'\\U0001F460',\n",
    "    u':high-speed_train:': u'\\U0001F684',\n",
    "    u':high-speed_train_with_bullet_nose:': u'\\U0001F685',\n",
    "    u':high_voltage:': u'\\U000026A1',\n",
    "    u':honey_pot:': u'\\U0001F36F',\n",
    "    u':honeybee:': u'\\U0001F41D',\n",
    "    u':horizontal_traffic_light:': u'\\U0001F6A5',\n",
    "    u':horse:': u'\\U0001F40E',\n",
    "    u':hospital:': u'\\U0001F3E5',\n",
    "    u':hot_beverage:': u'\\U00002615',\n",
    "    u':hot_dog:': u'\\U0001F32D',\n",
    "    u':hot_pepper:': u'\\U0001F336',\n",
    "    u':hot_springs:': u'\\U00002668',\n",
    "    u':hotel:': u'\\U0001F3E8',\n",
    "    u':hourglass:': u'\\U0000231B',\n",
    "    u':hourglass_with_flowing_sand:': u'\\U000023F3',\n",
    "    u':house:': u'\\U0001F3E0',\n",
    "    u':house_with_garden:': u'\\U0001F3E1',\n",
    "    u':hugging_face:': u'\\U0001F917',\n",
    "    u':hundred_points:': u'\\U0001F4AF',\n",
    "    u':hushed_face:': u'\\U0001F62F',\n",
    "    u':ice_cream:': u'\\U0001F368',\n",
    "    u':ice_hockey:': u'\\U0001F3D2',\n",
    "    u':ice_skate:': u'\\U000026F8',\n",
    "    u':inbox_tray:': u'\\U0001F4E5',\n",
    "    u':incoming_envelope:': u'\\U0001F4E8',\n",
    "    u':index_pointing_up:': u'\\U0000261D',\n",
    "    u':kick_scooter:': u'\\U0001F6F4',\n",
    "    u':kimono:': u'\\U0001F458',\n",
    "    u':kiss:': u'\\U0001F48F',\n",
    "    u':koala:': u'\\U0001F428',\n",
    "    u':label:': u'\\U0001F3F7',\n",
    "    u':lady_beetle:': u'\\U0001F41E',\n",
    "    u':leopard:': u'\\U0001F406',\n",
    "    u':level_slider:': u'\\U0001F39A',\n",
    "    u':locomotive:': u'\\U0001F682',\n",
    "    u':loudly_crying_face:': u'\\U0001F62D',\n",
    "    u':map_of_Japan:': u'\\U0001F5FE',\n",
    "    u':meat_on_bone:': u'\\U0001F356',\n",
    "    u':medical_symbol:': u'\\U00002695',\n",
    "    u':mount_fuji:': u'\\U0001F5FB',\n",
    "    u':mountain:': u'\\U000026F0',\n",
    "    u':mountain_cableway:': u'\\U0001F6A0',\n",
    "    u':mountain_railway:': u'\\U0001F69E',\n",
    "    u':mouth:': u'\\U0001F444',\n",
    "    u':necktie:': u'\\U0001F454',\n",
    "    u':nerd_face:': u'\\U0001F913',\n",
    "    u':neutral_face:': u'\\U0001F610',\n",
    "    u':no_smoking:': u'\\U0001F6AD',\n",
    "    u':non-potable_water:': u'\\U0001F6B1',\n",
    "    u':open_book:': u'\\U0001F4D6',\n",
    "    u':open_file_folder:': u'\\U0001F4C2',\n",
    "    u':open_hands:': u'\\U0001F450',\n",
    "    u':package:': u'\\U0001F4E6',\n",
    "    u':page_facing_up:': u'\\U0001F4C4',\n",
    "    u':page_with_curl:': u'\\U0001F4C3',\n",
    "    u':peace_symbol:': u'\\U0000262E',\n",
    "    u':pen:': u'\\U0001F58A',\n",
    "    u':pick:': u'\\U000026CF',\n",
    "    u':potable_water:': u'\\U0001F6B0',\n",
    "    u':pouting_face:': u'\\U0001F621',\n",
    "    u':prohibited:': u'\\U0001F6AB',\n",
    "    u':purple_heart:': u'\\U0001F49C',\n",
    "    u':question_mark:': u'\\U00002753',\n",
    "    u':rabbit:': u'\\U0001F407',\n",
    "    u':rabbit_face:': u'\\U0001F430',\n",
    "    u':radioactive:': u'\\U00002622',\n",
    "    u':railway_car:': u'\\U0001F683',\n",
    "    u':railway_track:': u'\\U0001F6E4',\n",
    "    u':rainbow:': u'\\U0001F308',\n",
    "    u':rainbow_flag:': u'\\U0001F3F3 \\U0000FE0F \\U0000200D \\U0001F308',\n",
    "    u':raised_back_of_hand:': u'\\U0001F91A',\n",
    "    u':raised_fist:': u'\\U0000270A',\n",
    "    u':raised_hand:': u'\\U0000270B',\n",
    "    u':raised_hand_with_fingers_splayed:': u'\\U0001F590',\n",
    "    u':raising_hands:': u'\\U0001F64C',\n",
    "    u':record_button:': u'\\U000023FA',\n",
    "    u':recycling_symbol:': u'\\U0000267B',\n",
    "    u':red_heart:': u'\\U00002764',\n",
    "    u':relieved_face:': u'\\U0001F60C',\n",
    "    u':roller_coaster:': u'\\U0001F3A2',\n",
    "    u':rolling_on_the_floor_laughing:': u'\\U0001F923',\n",
    "    u':shallow_pan_of_food:': u'\\U0001F958',\n",
    "    u':shamrock:': u'\\U00002618',\n",
    "    u':shark:': u'\\U0001F988',\n",
    "    u':sign_of_the_horns:': u'\\U0001F918',\n",
    "    u':skull:': u'\\U0001F480',\n",
    "    u':skull_and_crossbones:': u'\\U00002620',\n",
    "    u':sleeping_face:': u'\\U0001F634',\n",
    "    u':sleepy_face:': u'\\U0001F62A',\n",
    "    u':slightly_frowning_face:': u'\\U0001F641',\n",
    "    u':slightly_smiling_face:': u'\\U0001F642',\n",
    "    u':smiling_face:': u'\\U0000263A',\n",
    "    u':smiling_face_with_halo:': u'\\U0001F607',\n",
    "    u':smiling_face_with_heart-eyes:': u'\\U0001F60D',\n",
    "    u':smiling_face_with_horns:': u'\\U0001F608',\n",
    "    u':smiling_face_with_open_mouth:': u'\\U0001F603',\n",
    "    u':smiling_face_with_open_mouth_&_closed_eyes:': u'\\U0001F606',\n",
    "    u':smiling_face_with_open_mouth_&_cold_sweat:': u'\\U0001F605',\n",
    "    u':smiling_face_with_open_mouth_&_smiling_eyes:': u'\\U0001F604',\n",
    "    u':smiling_face_with_smiling_eyes:': u'\\U0001F60A',\n",
    "    u':smiling_face_with_sunglasses:': u'\\U0001F60E',\n",
    "    u':smirking_face:': u'\\U0001F60F',\n",
    "    u':snail:': u'\\U0001F40C',\n",
    "    u':snake:': u'\\U0001F40D',\n",
    "    u':sneezing_face:': u'\\U0001F927',\n",
    "    u':snow-capped_mountain:': u'\\U0001F3D4',\n",
    "    u':snowboarder:': u'\\U0001F3C2',\n",
    "    u':snowman:': u'\\U00002603',\n",
    "    u':snowman_without_snow:': u'\\U000026C4',\n",
    "    u':sparkling_heart:': u'\\U0001F496',\n",
    "    u':speak-no-evil_monkey:': u'\\U0001F64A',\n",
    "    u':speaker_high_volume:': u'\\U0001F50A',\n",
    "    u':speaker_low_volume:': u'\\U0001F508',\n",
    "    u':speaker_medium_volume:': u'\\U0001F509',\n",
    "    u':speaking_head:': u'\\U0001F5E3',\n",
    "    u':speech_balloon:': u'\\U0001F4AC',\n",
    "    u':speedboat:': u'\\U0001F6A4',\n",
    "    u':spider:': u'\\U0001F577',\n",
    "    u':spider_web:': u'\\U0001F578',\n",
    "    u':spiral_calendar:': u'\\U0001F5D3',\n",
    "    u':spiral_notepad:': u'\\U0001F5D2',\n",
    "    u':spiral_shell:': u'\\U0001F41A',\n",
    "    u':spoon:': u'\\U0001F944',\n",
    "    u':sport_utility_vehicle:': u'\\U0001F699',\n",
    "    u':spouting_whale:': u'\\U0001F433',\n",
    "    u':stop_button:': u'\\U000023F9',\n",
    "    u':stop_sign:': u'\\U0001F6D1',\n",
    "    u':stopwatch:': u'\\U000023F1',\n",
    "    u':straight_ruler:': u'\\U0001F4CF',\n",
    "    u':strawberry:': u'\\U0001F353',\n",
    "    u':studio_microphone:': u'\\U0001F399',\n",
    "    u':stuffed_flatbread:': u'\\U0001F959',\n",
    "    u':sun:': u'\\U00002600',\n",
    "    u':sun_behind_cloud:': u'\\U000026C5',\n",
    "    u':sun_behind_large_cloud:': u'\\U0001F325',\n",
    "    u':sun_behind_rain_cloud:': u'\\U0001F326',\n",
    "    u':sun_behind_small_cloud:': u'\\U0001F324',\n",
    "    u':sun_with_face:': u'\\U0001F31E',\n",
    "    u':sunrise:': u'\\U0001F305',\n",
    "    u':sunrise_over_mountains:': u'\\U0001F304',\n",
    "    u':sunset:': u'\\U0001F307',\n",
    "    u':suspension_railway:': u'\\U0001F69F',\n",
    "    u':sweat_droplets:': u'\\U0001F4A6',\n",
    "    u':teacup_without_handle:': u'\\U0001F375',\n",
    "    u':tear-off_calendar:': u'\\U0001F4C6',\n",
    "    u':telephone:': u'\\U0000260E',\n",
    "    u':telephone_receiver:': u'\\U0001F4DE',\n",
    "    u':telescope:': u'\\U0001F52D',\n",
    "    u':television:': u'\\U0001F4FA',\n",
    "    u':ten-thirty:': u'\\U0001F565',\n",
    "    u':ten_o’clock:': u'\\U0001F559',\n",
    "    u':tennis:': u'\\U0001F3BE',\n",
    "    u':tent:': u'\\U000026FA',\n",
    "    u':thermometer:': u'\\U0001F321',\n",
    "    u':thinking_face:': u'\\U0001F914',\n",
    "    u':thought_balloon:': u'\\U0001F4AD',\n",
    "    u':three-thirty:': u'\\U0001F55E',\n",
    "    u':three_o’clock:': u'\\U0001F552',\n",
    "    u':thumbs_down:': u'\\U0001F44E',\n",
    "    u':thumbs_up:': u'\\U0001F44D',\n",
    "    u':ticket:': u'\\U0001F3AB',\n",
    "    u':tiger:': u'\\U0001F405',\n",
    "    u':tiger_face:': u'\\U0001F42F',\n",
    "    u':timer_clock:': u'\\U000023F2',\n",
    "    u':tired_face:': u'\\U0001F62B',\n",
    "    u':toilet:': u'\\U0001F6BD',\n",
    "    u':tomato:': u'\\U0001F345',\n",
    "    u':tongue:': u'\\U0001F445',\n",
    "    u':top_hat:': u'\\U0001F3A9',\n",
    "    u':tornado:': u'\\U0001F32A',\n",
    "    u':trackball:': u'\\U0001F5B2',\n",
    "    u':tractor:': u'\\U0001F69C',\n",
    "    u':trade_mark:': u'\\U00002122',\n",
    "    u':train:': u'\\U0001F686',\n",
    "    u':tram:': u'\\U0001F68A',\n",
    "    u':tram_car:': u'\\U0001F68B',\n",
    "    u':triangular_flag:': u'\\U0001F6A9',\n",
    "    u':triangular_ruler:': u'\\U0001F4D0',\n",
    "    u':trident_emblem:': u'\\U0001F531',\n",
    "    u':trolleybus:': u'\\U0001F68E',\n",
    "    u':trophy:': u'\\U0001F3C6',\n",
    "    u':tropical_drink:': u'\\U0001F379',\n",
    "    u':tropical_fish:': u'\\U0001F420',\n",
    "    u':trumpet:': u'\\U0001F3BA',\n",
    "    u':tulip:': u'\\U0001F337',\n",
    "    u':tumbler_glass:': u'\\U0001F943',\n",
    "    u':turkey:': u'\\U0001F983',\n",
    "    u':turtle:': u'\\U0001F422',\n",
    "    u':twelve-thirty:': u'\\U0001F567',\n",
    "    u':twelve_o’clock:': u'\\U0001F55B',\n",
    "    u':two-hump_camel:': u'\\U0001F42B',\n",
    "    u':two-thirty:': u'\\U0001F55D',\n",
    "    u':two_hearts:': u'\\U0001F495',\n",
    "    u':umbrella:': u'\\U00002602',\n",
    "    u':umbrella_on_ground:': u'\\U000026F1',\n",
    "    u':umbrella_with_rain_drops:': u'\\U00002614',\n",
    "    u':unamused_face:': u'\\U0001F612',\n",
    "    u':unicorn_face:': u'\\U0001F984',\n",
    "    u':unlocked:': u'\\U0001F513',\n",
    "    u':up-down_arrow:': u'\\U00002195',\n",
    "    u':up-left_arrow:': u'\\U00002196',\n",
    "    u':up-right_arrow:': u'\\U00002197',\n",
    "    u':up_arrow:': u'\\U00002B06',\n",
    "    u':up_button:': u'\\U0001F53C',\n",
    "    u':upside-down_face:': u'\\U0001F643',\n",
    "    u':vertical_traffic_light:': u'\\U0001F6A6',\n",
    "    u':vibration_mode:': u'\\U0001F4F3',\n",
    "    u':victory_hand:': u'\\U0000270C',\n",
    "    u':video_camera:': u'\\U0001F4F9',\n",
    "    u':video_game:': u'\\U0001F3AE',\n",
    "    u':videocassette:': u'\\U0001F4FC',\n",
    "    u':violin:': u'\\U0001F3BB',\n",
    "    u':volcano:': u'\\U0001F30B',\n",
    "    u':volleyball:': u'\\U0001F3D0',\n",
    "    u':vulcan_salute:': u'\\U0001F596',\n",
    "    u':waning_crescent_moon:': u'\\U0001F318',\n",
    "    u':waning_gibbous_moon:': u'\\U0001F316',\n",
    "    u':warning:': u'\\U000026A0',\n",
    "    u':wastebasket:': u'\\U0001F5D1',\n",
    "    u':watch:': u'\\U0000231A',\n",
    "    u':water_buffalo:': u'\\U0001F403',\n",
    "    u':water_closet:': u'\\U0001F6BE',\n",
    "    u':water_wave:': u'\\U0001F30A',\n",
    "    u':waving_hand:': u'\\U0001F44B',\n",
    "    u':wavy_dash:': u'\\U00003030',\n",
    "    u':waxing_crescent_moon:': u'\\U0001F312',\n",
    "    u':waxing_gibbous_moon:': u'\\U0001F314',\n",
    "    u':weary_cat_face:': u'\\U0001F640',\n",
    "    u':weary_face:': u'\\U0001F629',\n",
    "    u':whale:': u'\\U0001F40B',\n",
    "    u':wheel_of_dharma:': u'\\U00002638',\n",
    "    u':wheelchair_symbol:': u'\\U0000267F',\n",
    "    u':white_circle:': u'\\U000026AA',\n",
    "    u':white_exclamation_mark:': u'\\U00002755',\n",
    "    u':white_flag:': u'\\U0001F3F3',\n",
    "    u':white_flower:': u'\\U0001F4AE',\n",
    "    u':wilted_flower:': u'\\U0001F940',\n",
    "    u':wind_chime:': u'\\U0001F390',\n",
    "    u':wind_face:': u'\\U0001F32C',\n",
    "    u':wine_glass:': u'\\U0001F377',\n",
    "    u':winking_face:': u'\\U0001F609',\n",
    "    u':wolf_face:': u'\\U0001F43A',\n",
    "    u':world_map:': u'\\U0001F5FA',\n",
    "    u':worried_face:': u'\\U0001F61F',\n",
    "    u':yellow_heart:': u'\\U0001F49B',\n",
    "    u':yen_banknote:': u'\\U0001F4B4',\n",
    "    u':yin_yang:': u'\\U0000262F',\n",
    "    u':zipper-mouth_face:': u'\\U0001F910',\n",
    "    u':zzz:': u'\\U0001F4A4',\n",
    "}\n",
    "\n",
    "UNICODE_EMOJI = {v: k for k, v in EMO_UNICODE.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'HAHAHA this is so funny :face_with_tears_of_joy:'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will run a regex for every emoticon and every emoji to see if there is a match. In such a case, the emoji will be substituted by the textual representation\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        regex = r'('+emot+')'\n",
    "        text = re.sub(regex, UNICODE_EMOJI[emot], text)\n",
    "    return text\n",
    "\n",
    "text = \"HAHAHA this is so funny 😂\"\n",
    "convert_emojis(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is still something that needs to be done in order to achieve the desired outcome: let us remove the colons at the beginning and end of the transcription"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'HAHAHA this is so funny face with tears of joy'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        regex = r'('+emot+')'\n",
    "        text = re.sub(regex, ' '.join(UNICODE_EMOJI[emot].replace(':','').split('_')), text)\n",
    "    return text\n",
    "\n",
    "text = \"HAHAHA this is so funny 😂\"\n",
    "convert_emojis(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Emoticon handling\n",
    "Handling emoticons will be similar but just a bit more tricky. In fact, to some emoticons that are multiple meanings that can be associated to it.\n",
    "\n",
    "For example:\n",
    "\":‑D\"  --->   \"Laughing, big grin or laugh with glasses\"\n",
    "\n",
    "Since we want all the replacements to be done in the same way for emojis and emoticons, we will remove the commas and substitute all the spaces with underscores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, angry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, angry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, angry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, angry or pouting\",\n",
    "    u\":c\":\"Frown, sad, angry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, angry or pouting\",\n",
    "    u\":<\":\"Frown, sad, angry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, angry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, angry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, angry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, angry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, angry or pouting\",\n",
    "    u\":@\":\"Frown, sad, angry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, angry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'HAHAHA this is so funny laughing big grin or laugh with glasses'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emoticon(text):\n",
    "    for emot in EMOTICONS:\n",
    "        regex = r'('+emot+')'\n",
    "        text = re.sub(regex, EMOTICONS[emot].replace(':','').replace(',','').lower(), text)\n",
    "    return text\n",
    "\n",
    "text = \"HAHAHA this is so funny XD\"\n",
    "convert_emoticon(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we're done with our functions, we can perform the transformation over our dataset for both emojis and emoticons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(lambda x: convert_emojis(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(lambda x: convert_emoticon(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spellchecking\n",
    "\n",
    "Another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis.\n",
    "For example consider the example:\n",
    "\"I canqt be happy today\".\n",
    "Here the user wanted to say \"can't\", however because of a spelling error our model won't be able to understand that the user in unhappy: it will just see something that doesn't understand (canqt) and will draw its conclusions from the word happy (which is actually the negation of the meaning of the sentence).\n",
    "\n",
    "For this task, the package spello becomes really handy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spello in c:\\users\\user\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk<4,>=3.4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spello) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk<4,>=3.4.5->spello) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install spello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once this is installed, import the pretrained model for fixing the spelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<spello.model.SpellCorrectionModel at 0x1fbecdbc0a0>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spello.model import SpellCorrectionModel\n",
    "\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('./en.pkl/en.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try the spelling checker function to see how it actually works:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't be happy today\n"
     ]
    }
   ],
   "source": [
    "text = \"I canqt be happy today\"\n",
    "print(sp.spell_correct(text)['spell_corrected_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can run it over the whole dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(lambda x: sp.spell_correct(x)['spell_corrected_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling of contractions\n",
    "\n",
    "Once that we spelled checked our data we can take everything one step further:\n",
    "we will handle contractions by modifying them in their full form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Defining the dictionary of negations that we want to handle\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                 \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                 \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                 \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                 \"mustn't\":\"must not\"}\n",
    "\n",
    "# Compiling the regex for performance reasons\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def remove_contractions(text):\n",
    "    return neg_pattern.sub(lambda x: negations_dic[x.group()], text)\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: remove_contractions(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization of words\n",
    "\n",
    "Before saving the file, we want to transform the words obtained after our preprocessing to tokens since all the methods that we are going to use later will work directly on the words.\n",
    "\n",
    "The TweetTokenizer from the package nltk.tokenize seems to be a good match for our goal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "   sentiment                                            message  \\\n0         -1  [climate, change, interesting, hostile, global...   \n1          1  [watch, beforetheflood, right, travels, world,...   \n2          1  [fabulous, leonardo, decaprio, film, climate, ...   \n3          1  [watched, amazing, documentary, leonardodicapr...   \n9          1  [beforetheflood, watch, beforetheflood, right,...   \n\n                                    original_message  \n0  @tiniebeany climate change is an interesting h...  \n1  RT @NatGeoChannel: Watch #BeforeTheFlood right...  \n2  Fabulous! Leonardo #DiCaprio's film on #climat...  \n3  RT @Mick_Fanning: Just watched this amazing do...  \n9  #BeforeTheFlood Watch #BeforeTheFlood right he...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>original_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>[climate, change, interesting, hostile, global...</td>\n      <td>@tiniebeany climate change is an interesting h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[watch, beforetheflood, right, travels, world,...</td>\n      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[fabulous, leonardo, decaprio, film, climate, ...</td>\n      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>[watched, amazing, documentary, leonardodicapr...</td>\n      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>[beforetheflood, watch, beforetheflood, right,...</td>\n      <td>#BeforeTheFlood Watch #BeforeTheFlood right he...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "df['message'] = df['message'].apply(lambda x: tok.tokenize(x))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization of words\n",
    "\n",
    "Lemmatization is the process of transforming words to their base form (or lemma, as it is usually said in NLP).\n",
    "For example: \"loving\" => \"love\"\n",
    "\n",
    "This process is different from stemming which is simply removing the suffix of the word. I thought that using the lemma rather than a word without meaning may be helpful. Of course, the time needed for lemmatization is higher compared to stemming but by looking on the internet it seems to be the best choice in my case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform our task we need to download WordNet.\n",
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "\n",
    "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. We will use the Lemmatizer from this package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmat(w_list):\n",
    "    lemm_sentence = []\n",
    "    for w in w_list:\n",
    "        pos_tag = nltk.pos_tag([w])[0]\n",
    "        # Adjective\n",
    "        if pos_tag[1].startswith('J'):\n",
    "            wtag = wordnet.ADJ\n",
    "        # Noun\n",
    "        elif pos_tag[1].startswith('N'):\n",
    "            wtag = wordnet.NOUN\n",
    "        # Adverb\n",
    "        elif pos_tag[1].startswith('R'):\n",
    "            wtag = wordnet.ADV\n",
    "        # Verb\n",
    "        elif pos_tag[1].startswith('V'):\n",
    "            wtag = wordnet.VERB\n",
    "        # Default to noun\n",
    "        else:\n",
    "            wtag = wordnet.NOUN\n",
    "\n",
    "    # Lemmatize each word in tweet\n",
    "        lemmetized_word = lemmatizer.lemmatize(w, pos=wtag)\n",
    "        lemm_sentence.append(lemmetized_word)\n",
    "    return lemm_sentence\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: lemmat(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0        [climate, change, interest, hostile, global, w...\n1        [watch, beforetheflood, right, travel, world, ...\n2        [fabulous, leonardo, decaprio, film, climate, ...\n3        [watch, amaze, documentary, leonardodicaprio, ...\n9        [beforetheflood, watch, beforetheflood, right,...\n                               ...                        \n43935             [american, scar, clown, climate, change]\n43936    [aikbaatsunithi, global, warm, negative, effec...\n43938    [dear, yeah, right, human, mediate, climate, c...\n43939    [respective, pas, prevent, climate, change, gl...\n43942    [wealthy, fossil, fuel, industry, know, climat...\nName: message, Length: 24463, dtype: object"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For future use, we will also store the \"preprocessed version\" of the message as a string in a dedicated column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['message'].apply(lambda x: \" \".join(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving preprocessed data to a text file\n",
    "\n",
    "There are more things that we can do, but since they depend on the approach used, we will deal with it later. (I am talking about the generation of count vectors with Bag Of Words or TF-idf).\n",
    "\n",
    "For now, we just save our preprocessed data in a .csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
